# -*- coding: utf-8 -*-

import os
import feedparser
import requests
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta
import pytz
import json

# --- 1. é…ç½®åŒºåŸŸ ---
RSS_FEEDS = {
    "TechCrunch AI (EN)": "https://techcrunch.com/category/artificial-intelligence/feed/",
    "é‡å­ä½ (ä¸­æ–‡)": "https://www.qbitai.com/feed/",
    "æœºå™¨ä¹‹å¿ƒ (ä¸­æ–‡)": "https://www.jiqizhixin.com/rss",
    "Reuters World (EN)": "https://www.reuters.com/world/rss/",
    "BBC World (EN)": "http://feeds.bbci.co.uk/news/world/rss.xml",
    "NYT World (EN)": "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "MIT Tech Review (EN)": "https://www.technologyreview.com/c/artificial-intelligence/feed/",
    "ArXiv CS.AI (Paper)": "http://arxiv.org/rss/cs.AI"
}

PER_FEED_LIMIT = 4
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- 2. åŠŸèƒ½å‡½æ•° ---

def get_balanced_articles(feed_urls, limit_per_feed):
    print("ğŸš€ å¼€å§‹ä»å„æ–°é—»æºå‡è¡¡è·å–æ–‡ç« ...")
    all_articles = []
    unique_links = set()
    utc = pytz.UTC
    twenty_four_hours_ago = datetime.now(utc) - timedelta(hours=24)
    for name, url in feed_urls.items():
        try:
            feed = feedparser.parse(url)
            print(f"  - æ­£åœ¨å¤„ç†: {name}")
            count = 0
            for entry in feed.entries:
                if count >= limit_per_feed: break
                published_time = None
                if 'published_parsed' in entry and entry.published_parsed:
                    published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed), utc)
                if published_time and published_time > twenty_four_hours_ago and entry.link not in unique_links:
                    article_data = { 'title': entry.title, 'link': entry.link, 'source': name, 'summary': entry.get('summary', '') }
                    all_articles.append(article_data)
                    unique_links.add(entry.link)
                    count += 1
        except Exception as e:
            print(f"  âŒ è·å– {name} æ—¶å‡ºé”™: {e}")
    print(f"âœ… è·å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_articles)} æ¡æ–°é—»ã€‚")
    return all_articles

def get_content_with_playwright(url):
    content = None
    print("    - å¯åŠ¨Playwrightæµè§ˆå™¨...")
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.set_extra_http_headers({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"})
            page.goto(url, wait_until='domcontentloaded', timeout=30000)
            page.wait_for_timeout(3000)
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            main_content = soup.find('article') or soup.find('main')
            if main_content:
                paragraphs = main_content.find_all('p')
            else:
                paragraphs = soup.find_all('p')
            content = "\n".join([p.get_text() for p in paragraphs if p.get_text()])
            browser.close()
            if content: print("    - Playwright æå–æˆåŠŸï¼")
            else: print("    - Playwright æå–å†…å®¹ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"    - Playwright æå–å¤±è´¥: {e}")
    return content[:3000] if content else None

def summarize_with_gemini_direct(content, api_key):
    """
    å†³å®šæ€§æ–¹æ¡ˆï¼šç›´æ¥é€šè¿‡requestsæ‰‹åŠ¨è°ƒç”¨v1 APIï¼Œå¹¶ä½¿ç”¨å®˜æ–¹æ¨èçš„æœ€æ–°æ¨¡å‹åç§°ã€‚
    """
    if not content:
        return "æ— æ³•è·å–æ­£æ–‡ï¼Œè·³è¿‡æ€»ç»“ã€‚"
    
    # *** å†³å®šæ€§çš„æœ€ç»ˆä¿®æ”¹ï¼šä½¿ç”¨å®˜æ–¹æ¨èçš„ã€æ›´ç°ä»£çš„æ¨¡å‹åç§° ***
    model_name = "gemini-1.5-flash-latest"
    api_url = f"https://generativelanguage.googleapis.com/v1/models/{model_name}:generateContent?key={api_key}"
    
    headers = {'Content-Type': 'application/json'}
    prompt = f"è¯·ç”¨ç®€ä½“ä¸­æ–‡ï¼Œç”¨ä¸€å¥è¯ï¼ˆä¸è¶…è¿‡60å­—ï¼‰ç²¾å‡†åœ°æ€»ç»“ä»¥ä¸‹æ–°é—»æŠ¥é“æˆ–è®ºæ–‡æ‘˜è¦çš„æ ¸å¿ƒå†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•å¤šä½™çš„å¼€å¤´æˆ–ç»“å°¾ï¼š\n\n---\n{content}\n---"
    data = {"contents": [{"parts": [{"text": prompt}]}]}

    try:
        response = requests.post(api_url, headers=headers, data=json.dumps(data), timeout=30)
        response.raise_for_status()
        response_json = response.json()
        summary = response_json['candidates'][0]['content']['parts'][0]['text']
        return summary.strip().replace('*', '')
    except requests.exceptions.RequestException as e:
        print(f"    - Gemini APIè¯·æ±‚å¤±è´¥ (ç½‘ç»œå±‚é¢): {e}")
        if e.response is not None:
            print(f"    - å“åº”å†…å®¹: {e.response.text}")
        return "AIæ€»ç»“å¤±è´¥ã€‚"
    except (KeyError, IndexError) as e:
        print(f"    - è§£æGemini APIå“åº”å¤±è´¥: {e}, å“åº”å†…å®¹: {response.text}")
        return "AIæ€»ç»“å¤±è´¥ã€‚"
    except Exception as e:
        print(f"    - è°ƒç”¨Gemini APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return "AIæ€»ç»“å¤±è´¥ã€‚"

def send_to_feishu(content):
    if not content:
        print("å†…å®¹ä¸ºç©ºï¼Œä¸å‘é€æ¶ˆæ¯ã€‚")
        return
    headers = {'Content-Type': 'application/json'}
    payload = { "msg_type": "interactive", "card": { "header": { "title": { "tag": "plain_text", "content": f"ğŸ”” ä»Šæ—¥æ–°é—»æ‘˜è¦ ({datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')})" }, "template": "blue" }, "elements": [ { "tag": "div", "text": { "tag": "lark_md", "content": content }}, {"tag": "hr"}, { "tag": "note", "elements": [{"tag": "plain_text", "content": "ç”±GitHub Actions + Gemini Pro é©±åŠ¨"}] } ] } }
    try:
        response = requests.post(FEISHU_WEBHOOK_URL, json=payload, headers=headers)
        if response.status_code == 200 and response.json().get("StatusCode") == 0: print("ğŸ‰ æˆåŠŸå‘é€æ¶ˆæ¯åˆ°é£ä¹¦ï¼")
        else: print(f"âŒ å‘é€é£ä¹¦å¤±è´¥: {response.status_code}, {response.text}")
    except Exception as e: print(f"âŒ å‘é€é£ä¹¦æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")

# --- 3. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    if not FEISHU_WEBHOOK_URL or not GEMINI_API_KEY:
        print("ğŸš¨ é”™è¯¯ï¼šæœªè®¾ç½® FEISHU_WEBHOOK_URL æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡ï¼")
        exit()
    articles = get_balanced_articles(RSS_FEEDS, PER_FEED_LIMIT)
    if not articles:
        print("ğŸ’¤ ä»Šå¤©æ²¡æœ‰å‘ç°æ–°æ–‡ç« ï¼Œç¨‹åºç»“æŸã€‚")
        exit()
    print("\nğŸ” å¼€å§‹å¤„ç†æ¯ç¯‡æ–‡ç« å¹¶ç”Ÿæˆæ‘˜è¦...")
    summaries = []
    for i, article in enumerate(articles):
        print(f"  - ({i+1}/{len(articles)}) æ­£åœ¨å¤„ç†: {article['title']}")
        summary = ""
        if 'ArXiv' in article['source'] and article['summary']:
            print("    - æ£€æµ‹åˆ°ArXivé“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨è‡ªå¸¦æ‘˜è¦ã€‚")
            soup = BeautifulSoup(article['summary'], 'html.parser')
            summary = soup.get_text().strip().replace('\n', ' ')
        else:
            content = get_content_with_playwright(article['link'])
            summary = summarize_with_gemini_direct(content, GEMINI_API_KEY)
        formatted_item = ( f"**{article['title']}**\n" f"> **æ‘˜è¦**: {summary}\n" f"æ¥æº: {article['source']}\n" f"é“¾æ¥: [{article['link']}]({article['link']})\n" )
        summaries.append(formatted_item)
        time.sleep(1)
    if summaries:
        final_content = "\n---\n\n".join(summaries)
        send_to_feishu(final_content)
    else:
        print("æ‰€æœ‰æ–‡ç« éƒ½æœªèƒ½æˆåŠŸç”Ÿæˆæ‘˜è¦ï¼Œä¸å‘é€æ¶ˆæ¯ã€‚")
    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
