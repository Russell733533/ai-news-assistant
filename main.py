# -*- coding: utf-8 -*-

import os
import feedparser
import requests
import google.generativeai as genai
from newspaper import Article, Config
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta
import pytz

# --- 1. é…ç½®åŒºåŸŸ ---
RSS_FEEDS = {
    "Google News AI (EN)": "https://news.google.com/rss/search?q=Artificial+Intelligence&hl=en-US&gl=US&ceid=US:en",
    "TechCrunch AI (EN)": "https://techcrunch.com/category/artificial-intelligence/feed/",
    "é‡å­ä½ (ä¸­æ–‡)": "https://www.qbitai.com/feed/",
    "æœºå™¨ä¹‹å¿ƒ (ä¸­æ–‡)": "https://www.jiqizhixin.com/rss",
    "MIT Tech Review (EN)": "https://www.technologyreview.com/c/artificial-intelligence/feed/",
    "ArXiv CS.AI (Paper)": "http://arxiv.org/rss/cs.AI"
}

PER_FEED_LIMIT = 5 

FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- 2. åŠŸèƒ½å‡½æ•° ---

def get_balanced_articles(feed_urls, limit_per_feed):
    """ä»æ¯ä¸ªRSSæºåˆ†åˆ«è·å–æŒ‡å®šæ•°é‡çš„æœ€æ–°æ–‡ç« ï¼Œå¹¶åŒ…å«æ‘˜è¦ï¼ˆé’ˆå¯¹ArXivï¼‰"""
    print("ğŸš€ å¼€å§‹ä»å„æ–°é—»æºå‡è¡¡è·å–æ–‡ç« ...")
    all_articles = []
    unique_links = set()
    
    utc = pytz.UTC
    twenty_four_hours_ago = datetime.now(utc) - timedelta(hours=24)

    for name, url in feed_urls.items():
        try:
            feed = feedparser.parse(url)
            print(f"  - æ­£åœ¨å¤„ç†: {name}")
            count = 0
            for entry in feed.entries:
                if count >= limit_per_feed: break
                
                published_time = None
                if 'published_parsed' in entry and entry.published_parsed:
                    published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed), utc)

                if published_time and published_time > twenty_four_hours_ago and entry.link not in unique_links:
                    article_data = {
                        'title': entry.title,
                        'link': entry.link,
                        'source': name,
                        'summary': entry.get('summary', '') # å…³é”®ï¼šè·å–RSSè‡ªå¸¦çš„æ‘˜è¦
                    }
                    all_articles.append(article_data)
                    unique_links.add(entry.link)
                    count += 1
        except Exception as e:
            print(f"  âŒ è·å– {name} æ—¶å‡ºé”™: {e}")
            
    print(f"âœ… è·å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_articles)} æ¡æ–°é—»ã€‚")
    return all_articles

def get_article_content_robust(article_data):
    """
    æ›´å¼ºå¤§çš„æ­£æ–‡è·å–å‡½æ•°ï¼Œæ™ºèƒ½å°è¯•å¤šç§æ–¹æ³•
    """
    url = article_data['link']
    
    # Plan B (ç‰¹æ®Šé€šé“ for ArXiv): ç›´æ¥ä½¿ç”¨RSSè‡ªå¸¦çš„æ‘˜è¦
    if 'ArXiv' in article_data['source']:
        print(f"    - æ£€æµ‹åˆ°ArXivé“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨æ‘˜è¦ã€‚")
        # ArXivæ‘˜è¦æœ¬èº«å°±æ˜¯HTMLæ ¼å¼ï¼Œéœ€è¦æ¸…ç†ä¸€ä¸‹
        soup = BeautifulSoup(article_data['summary'], 'html.parser')
        return soup.get_text()

    # Plan A (ä¸“å®¶ä¼˜å…ˆ): å°è¯•ä½¿ç”¨newspaper3k
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
        config.request_timeout = 15
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        if article.text:
            print(f"    - newspaper3k æå–æˆåŠŸã€‚")
            return article.text[:2500]
    except Exception as e:
        print(f"    - newspaper3k æå–å¤±è´¥: {e}")

    # Plan C (å¤‡ç”¨é’¥åŒ™): newspaper3kå¤±è´¥åï¼Œå°è¯•ä½¿ç”¨requests+BeautifulSoup
    print(f"    - newspaper3kå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        content = "\n".join([p.get_text() for p in paragraphs])
        if content:
            print(f"    - å¤‡ç”¨æ–¹æ³•æå–æˆåŠŸã€‚")
            return content[:2500]
    except Exception as e:
        print(f"    - å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†: {e}")

    return None # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†

def summarize_with_gemini(content):
    """ä½¿ç”¨Geminiè¿›è¡Œå†…å®¹æ€»ç»“"""
    if not content:
        return "æ— æ³•è·å–æ­£æ–‡ï¼Œè·³è¿‡æ€»ç»“ã€‚"
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"è¯·ç”¨ç®€ä½“ä¸­æ–‡ï¼Œç”¨ä¸€å¥è¯ï¼ˆä¸è¶…è¿‡50å­—ï¼‰ç²¾å‡†åœ°æ€»ç»“ä»¥ä¸‹æ–°é—»æˆ–è®ºæ–‡æ‘˜è¦çš„æ ¸å¿ƒå†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•å¤šä½™çš„å¼€å¤´æˆ–ç»“å°¾ï¼š\n\n---\n{content}\n---"
        response = model.generate_content(prompt)
        summary = response.text.strip().replace('*', '')
        return summary
    except Exception as e:
        print(f"    - Gemini APIè°ƒç”¨å¤±è´¥: {e}")
        return "AIæ€»ç»“å¤±è´¥ã€‚"

def send_to_feishu(content):
    """å°†æœ€ç»ˆæ ¼å¼åŒ–çš„å†…å®¹é€šè¿‡Webhookå‘é€åˆ°é£ä¹¦"""
    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä»£ç çœç•¥ä»¥ä¿æŒç®€æ´ï¼Œå®é™…ç²˜è´´æ—¶è¯·åŒ…å«å®Œæ•´ä»£ç ) ...
    if not content:
        print("å†…å®¹ä¸ºç©ºï¼Œä¸å‘é€æ¶ˆæ¯ã€‚")
        return
    headers = {'Content-Type': 'application/json'}
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {
                    "tag": "plain_text",
                    "content": f"ğŸ”” ä»Šæ—¥AIæ–°é—»æ‘˜è¦ ({datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')})"
                },
                "template": "blue"
            },
            "elements": [
                { "tag": "div", "text": { "tag": "lark_md", "content": content }},
                {"tag": "hr"},
                { "tag": "note", "elements": [{"tag": "plain_text", "content": "ç”±GitHub Actions + Gemini Pro é©±åŠ¨"}] }
            ]
        }
    }
    try:
        response = requests.post(FEISHU_WEBHOOK_URL, json=payload, headers=headers)
        if response.status_code == 200 and response.json().get("StatusCode") == 0:
            print("ğŸ‰ æˆåŠŸå‘é€æ¶ˆæ¯åˆ°é£ä¹¦ï¼")
        else:
            print(f"âŒ å‘é€é£ä¹¦å¤±è´¥: {response.status_code}, {response.text}")
    except Exception as e:
        print(f"âŒ å‘é€é£ä¹¦æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")


# --- 3. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    
    if not FEISHU_WEBHOOK_URL or not GEMINI_API_KEY:
        print("ğŸš¨ é”™è¯¯ï¼šæœªè®¾ç½® FEISHU_WEBHOOK_URL æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡ï¼")
        exit()

    articles = get_balanced_articles(RSS_FEEDS, PER_FEED_LIMIT)
    
    if not articles:
        print("ğŸ’¤ ä»Šå¤©æ²¡æœ‰å‘ç°æ–°æ–‡ç« ï¼Œç¨‹åºç»“æŸã€‚")
        exit()
        
    print("\nğŸ” å¼€å§‹å¤„ç†æ¯ç¯‡æ–‡ç« å¹¶ç”Ÿæˆæ‘˜è¦...")
    summaries = []
    for i, article in enumerate(articles):
        print(f"  - ({i+1}/{len(articles)}) æ­£åœ¨å¤„ç†: {article['title']}")
        
        # *** å…³é”®ä¿®æ”¹ï¼šè°ƒç”¨æ›´å¼ºå¤§çš„æ­£æ–‡è·å–å‡½æ•° ***
        content = get_article_content_robust(article)
        summary = summarize_with_gemini(content)
        
        formatted_item = (
            f"**{article['title']}**\n"
            f"> **æ‘˜è¦**: {summary}\n"
            f"æ¥æº: {article['source']}\n"
            f"é“¾æ¥: [{article['link']}]({article['link']})\n"
        )
        summaries.append(formatted_item)
        time.sleep(1)

    if summaries:
        final_content = "\n---\n\n".join(summaries)
        send_to_feishu(final_content)
    else:
        print("æ‰€æœ‰æ–‡ç« éƒ½æœªèƒ½æˆåŠŸç”Ÿæˆæ‘˜è¦ï¼Œä¸å‘é€æ¶ˆæ¯ã€‚")

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
